bts<-read.csv("C:/Users/OWNER/Downloads/news_comment_BTS.csv")

library(dplyr)
library(stringr)
library(textclean)
bts1 <- bts %>%
  select(reply) %>%
  mutate(reply = str_replace_all(reply, "[^가-힣]", " "),
         reply = str_squish(reply),
         id = row_number())

library(tidytext)
library(KoNLP)
bts2 <- bts1 %>%
  unnest_tokens(input = reply,
                output = word,
                token = SimplePos22,
                drop = F)
bts2 %>%
  select(word,reply)

# 품사별로 행 분리
library(tidyr)
bts2 <- bts2 %>%
  separate_rows(word, sep = "[+]")  # +가 있으면 밑으로 내려줘

bts2%>% 
  select(word, reply)

# 명사 추출하기
b <- bts2 %>%
  filter(str_detect(word, "/n")) %>%   # n다음에 있는거 지우기
  mutate(word = str_remove(word, "/.*$"))  # *아무숫자

b %>%
  select(word, reply)

b %>%
  count(word, sort = T)


# 동사, 형용사 추출하기
bb <- bts2 %>%
  filter(str_detect(word, "/pv|/pa")) %>%         # "/pv", "/pa" 추출(pv동사 pa형용사)/|합집합
  mutate(word = str_replace(word, "/.*$", "다"))  # "/"로 시작 문자를 "다"로 바꾸기

bb %>%
  select(word, reply)

bb %>%
  count(word, sort = T)

# 품사 결합
bt <- bind_rows(b, bb) %>% #noun명사만저장,pvpa동사형용사만
  filter(str_count(word) >= 2) %>%
  arrange(id)

bt %>%
  select(word, reply)

#명사, 동사, 형용사를 한번에 추출하기
#명사, 동사, 형용사를 추출해 결합한 후 두 글자 이상만 남기기
bts1 <- bts2 %>%
  separate_rows(word, sep = "[+]") %>%
  filter(str_detect(word, "/n|/pv|/pa")) %>%
  mutate(word = ifelse(str_detect(word, "/pv|/pa"),
                       str_replace(word, "/.*$", "다"), #동사 형용사 뒤에 다 붙이기
                       str_remove(word, "/.*$"))) %>% #명사 뒤에 지우기
  filter(str_count(word) >= 2) %>%
  arrange(id)


#단어 동시 출현 빈도 구하기
install.packages("widyr")
library(widyr)

pair <- bt %>%
  pairwise_count(item = word,
                 feature = id,
                 sort = T)
pair

pair %>% filter(item1 == "방탄소년단")

pair %>% filter(item1 == "빌보드")



install.packages("tidygraph")
library(tidygraph)

graph_comment <- pair %>%  #빈도수가 큰거만 그리기
  filter(n >= 10) %>%     #n >= 25 숫자 너무 작게하면 복잡해짐
  as_tbl_graph()        #as_tbl_graph()로 바꿔둬야 그래프 그려짐

graph_comment

install.packages("ggraph")
library(ggraph)

ggraph(graph_comment) +
  geom_edge_link() +                 # 엣지
  geom_node_point() +                # 노드
  geom_node_text(aes(label = name))  # 텍스트

library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()

#그림 좀 더 이쁘게 그리기
set.seed(1234)                              # 난수 고정
ggraph(graph_comment, layout = "fr") +      # 레이아웃(모양) (helep에서 ggraph치면 포함된 레이아웃들 나옴/구글 검색)
  
  geom_edge_link(color = "gray50",          # 엣지 색깔
                 alpha = 0.5,               # 엣지 명암
                 width=2) +                #선 굵기 지정 가능(굳이 필요 없음 기본값으로 해도 잘 나옴)
  
  geom_node_point(color = "lightcoral",     # 노드 색깔
                  size = 5) +               # 노드 크기
  
  geom_node_text(aes(label = name),         # 텍스트 표시
                 repel = T,                 # 노드밖 표시
                 size = 5,                  # 텍스트 크기
                 family = "nanumgothic") +  # 폰트
  
  theme_graph()                             # 배경 삭제


#네트워크 그래프 함수 만들다
word_network <- function(x) {
  ggraph(x, layout = "fr") +
    geom_edge_link(color = "gray50",
                   alpha = 0.5) +
    geom_node_point(color = "lightcoral",
                    size = 5) +
    geom_node_text(aes(label = name),
                   repel = T,
                   size = 5,
                   family = "nanumgothic") +
    theme_graph()
}

set.seed(1234)
word_network(graph_comment)

# 유의어 처리하기(비슷한 단어 동일하게 처리)
bt <- bt %>%
  mutate(word = ifelse(str_detect(word, "방탄"), "방탄소년단", word),
         word = ifelse(str_detect(word, "탄이들"), "방탄소년단", word), 
         word = ifelse(word == "진심", "진짜", word),          #오르다는 올리다
         word = ifelse(str_detect(word, "축하"), "축하", word),
         word = ifelse(str_detect(word, "자랑"), "자랑", word),
         word = ifelse(str_detect(word, "면제"),"군면제", word))


# 단어 동시 출현 빈도 구하기
pair <- bt %>%
  pairwise_count(item = word,
                 feature = id,
                 sort = T)


#네트워크 그래프 데이터에 연결 중심성, 커뮤니티 변수 추가하기
set.seed(1234)
graph_comment <- pair %>%
  filter(n >= 14) %>%
  as_tbl_graph(directed = F) %>%
  mutate(centrality = centrality_degree(),        # 연결 중심성(얼마만큼 겹쳐있는지)
         group = as.factor(group_infomap()))      # 커뮤니티

graph_comment


#네트워크 그래프 데이터에 연결 중심성, 커뮤니티 표현하기
set.seed(1234)
ggraph(graph_comment, layout = "fr") +      # 레이아웃
  
  geom_edge_link(color = "gray50",          # 엣지 색깔
                 alpha = 0.5) +             # 엣지 명암
  
  geom_node_point(aes(size = centrality,    # 노드 크기
                      color = group),       # 노드 색깔
                  show.legend = F) +        # 범례 삭제
  scale_size(range = c(5, 15)) +            # 노드 크기 범위
  
  geom_node_text(aes(label = name),         # 텍스트 표시
                 repel = T,                 # 노드밖 표시
                 size = 5,                  # 텍스트 크기
                 family = "nanumgothic") +  # 폰트
  
  theme_graph()                             # 배경 삭제



word_cors <- bt %>%
  add_count(word) %>%
  filter(n >= 14) %>%
  pairwise_cor(item = word,
               feature = id,
               sort = T)

word_cors



##파이 계수로 막대 그래프 만들기
#1. 관심 단어별로 파이 계수가 큰 단어 추출하기
# 관심 단어 목록 생성
target <- c("방탄소년단", "빌보드", "군면제", "축하", "대한민국","좋다")

top_cors <- word_cors %>%
  filter(item1 %in% target) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 8)

#2. 막대 그래프 만들기
# 그래프 순서 정하기
top_cors$item1 <- factor(top_cors$item1, levels = target)

library(ggplot2)
ggplot(top_cors, aes(x = reorder_within(item2, correlation, item1),
                     y = correlation,
                     fill = item1)) +
  geom_col(show.legend = F) +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))

##파이 계수로 네트워크 그래프 만들기
#1. 네트워크 그래프 데이터 만들기. 연결 중심성과 커뮤니티 추가하기
set.seed(1234)
graph_cors <- word_cors %>%
  filter(correlation >= 0.15) %>%
  as_tbl_graph(directed = F) %>%
  mutate(centrality = centrality_degree(),
         group = as.factor(group_infomap()))


set.seed(1234)
ggraph(graph_cors, layout = "fr") +
  
  geom_edge_link(color = "gray50",
                 aes(edge_alpha = correlation,   # 엣지 명암
                     edge_width = correlation),  # 엣지 두께(선이 두꺼우면 상관성 높고 얇으면 낮다)
                 show.legend = F) +              # 범례 삭제
  scale_edge_width(range = c(1, 4)) +            # 엣지 두께 범위
  
  geom_node_point(aes(size = centrality,
                      color = group),
                  show.legend = F) +
  scale_size(range = c(5, 10)) +
  
  geom_node_text(aes(label = name),
                 repel = T,
                 size = 5,
                 family = "nanumgothic") +
  
  theme_graph()



#기사 댓글로 바이그램 만들기
comment_new <- bts2 %>%
  separate_rows(word, sep = "[+]") %>%
  filter(str_detect(word, "/n|/pv|/pa")) %>%
  mutate(word = ifelse(str_detect(word, "/pv|/pa"),
                       str_replace(word, "/.*$", "다"),
                       str_remove(word, "/.*$"))) %>%
  filter(str_count(word) >= 2) %>%
  arrange(id)


comment_new <- comment_new %>%
  mutate(word = ifelse(str_detect(word, "방탄"), "방탄소년단", word),
         word = ifelse(str_detect(word, "탄이들"), "방탄소년단", word), 
         word = ifelse(word == "진심", "진짜", word),          #오르다는 올리다
         word = ifelse(str_detect(word, "축하"), "축하", word),
         word = ifelse(str_detect(word, "자랑"), "자랑", word),
         word = ifelse(str_detect(word, "면제"),"군면제", word))

comment_new %>%
  select(word)

#(3) 한 댓글이 하나의 행이 되도록 결합하기
line_comment <- comment_new %>%
  group_by(id) %>%
  summarise(sentence = paste(word, collapse = " "))

line_comment


#(4) 바이그램으로 토큰화하기
bigram_comment <- line_comment %>%
  unnest_tokens(input = sentence,
                output = bigram,
                token = "ngrams",
                n = 2)

bigram_comment

##연이어 사용된 단어쌍 빈도 구하기
# 1.바이그램 분리하기
bigram_seprated <- bigram_comment %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigram_seprated


# -------------------------------------------------------------------------
# 2.단어쌍 빈도 구하기
pair_bigram <- bigram_seprated %>%
  count(word1, word2, sort = T) %>%
  na.omit()

pair_bigram


# -------------------------------------------------------------------------
#3. 단어쌍 살펴보기
# 동시 출현 단어쌍
pair %>%
  filter(item1 == "대한민국")

# 바이그램 단어쌍
pair_bigram %>%
  filter(word1 == "대한민국")

# 동시 출현 단어쌍
pair %>%
  filter(item1 == "방탄소년단")

# 바이그램 단어쌍
pair_bigram %>%
  filter(word1 == "방탄소년단")

#엔그램으로 네트워크 그래프 만들기
# 네트워크 그래프 데이터 만들기
graph_bigram <- pair_bigram %>%
  filter(n >= 8) %>%
  as_tbl_graph()

# 네트워크 그래프 만들기
set.seed(1234)
word_network(graph_bigram)

#유의어 통일하고 네트워크 그래프 다시 만들기
bigram_seprated <- bigram_seprated %>%
  mutate(word1 = ifelse(str_detect(word1, "대단"), "대단", word1),
         word2 = ifelse(str_detect(word2, "대단"), "대단", word2),
         
         word1 = ifelse(str_detect(word1, "자랑"), "자랑", word1),
         word2 = ifelse(str_detect(word2, "자랑"), "자랑", word2),
         
         word1 = ifelse(str_detect(word1, "짝짝짝"), "짝짝짝", word1),
         word2 = ifelse(str_detect(word2, "짝짝짝"), "짝짝짝", word2)) %>%
  
  # 같은 단어 연속 제거
  filter(word1 != word2)

# 단어쌍 빈도 구하기
pair_bigram <- bigram_seprated %>%
  count(word1, word2, sort = T) %>%
  na.omit()

# 네트워크 그래프 만들기
set.seed(1234)
ggraph(graph_bigram, layout = "fr") +         # 레이아웃
  
  geom_edge_link(color = "gray50",            # 엣지 색깔
                 alpha = 0.5) +               # 엣지 명암
  
  geom_node_point(aes(size = centrality,      # 노드 크기
                      color = group),         # 노드 색깔
                  show.legend = F) +          # 범례 삭제
  scale_size(range = c(4, 8)) +               # 노드 크기 범위
  
  geom_node_text(aes(label = name),           # 텍스트 표시
                 repel = T,                   # 노드밖 표시
                 size = 5,                    # 텍스트 크기
                 family = "nanumgothic") +    # 폰트
  
  theme_graph()                               # 배경 삭제



# 단어 기준 토큰화
bt %>%
  unnest_tokens(input = value,
                output = word,
                token = "words")

# 유니그램 토큰화
bt %>%
  unnest_tokens(input = reply,
                output = word,
                token = "ngrams",
                n = 1)
