# 01-1 -------------------------------------------------------------------

raw_moon <- readLines("C:/Users/OWNER/Downloads/speech_moon.txt", encoding = "UTF-8")
head(raw_moon)

txt <- "치킨은!! 맛있다. xyz 정말 맛있다!@#"
txt

library(stringr)

str_replace_all(string = txt, pattern = "[^가-힣]", replacement = " ")

##[0-9],[a-z],[A-Z],[가-힣]

# ------------------------------------------------------------------------
moon <- raw_moon %>%
 str_replace_all("[^가-힣]", " ")

head(moon)


# ------------------------------------------------------------------------
# 파라미터명 입력
str_replace_all(string = txt, pattern = "[^가-힣]", replacement = " ")

# 파라미터명 생략
str_replace_all(txt, "[^가-힣]", " ")


# ------------------------------------------------------------------------
txt <- "치킨은  맛있다   정말 맛있다  "
txt
str_squish(txt)


# ------------------------------------------------------------------------
moon <- moon %>%
 str_squish()

head(moon)


# ------------------------------------------------------------------------
library(dplyr)
moon <- as_tibble(moon)
moon


# ------------------------------------------------------------------------
moon <- raw_moon %>%
 str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기
 str_squish() %>%                      # 연속된 공백 제거
 as_tibble()                           # tibble로 변환

## ex) yoon<-readLines("클립보드")
##ribrary(dplyr)
##yoon2 <- yoon %>%
##  str_replace_all("[^가-힣]", " ") %>%  
##  str_squish() %>%                      
##  as_tibble()             

# ------------------------------------------------------------------------
iris             # data frame 출력
as_tibble(iris)  # tibble 구조로 변환

#백터형식이라 티블로 바꿔줌, 장점은 앞에 10개만 나오고 나머지는 생략된다.
#데이터를 바꾼게 아닌 형식만 바꿈

# 01-2 --------------------------------------------------------------------

text <- tibble(value = "대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.")
text


# ------------------------------------------------------------------------
library(tidytext)

# 문장 기준 토큰화
text %>%
  unnest_tokens(input = value,        # 토큰화할 텍스트    #value는 변수명
                output = word,        # 출력 변수명
                token = "sentences")  # "sentences"문장 기준


# ------------------------------------------------------------------------
# 띄어쓰기 기준 토큰화
text %>%
  unnest_tokens(input = value,          
                output = word,
                token = "words")      # "words"띄어쓰기 기준(가장 쉬운 방법)


# ------------------------------------------------------------------------
# 문자 기준 토큰화
text %>%
  unnest_tokens(input = value,
                output = word,
                token = "characters")  # "characters"문자 기준


# 단어 빈도 구하기 -(count)------------------------------------------------
word_space <- moon %>%
  unnest_tokens(input = value,
                output = word,
                token = "words")
word_space

# 01-3 --------------------------------------------------------------------
word_space <- word_space %>%       #가장 많이 사용되는 단어들
 count(word, sort = T)

word_space


# ------------------------------------------------------------------------
str_count("배")
str_count("사과")


# ------------------------------------------------------------------------
# 두 글자 이상만 남기기
word_space <- word_space %>%      #글자 수를 세는 거
 filter(str_count(word) > 1)     #글자 수가 한개 이상인 것만 추출

word_space

# ------------------------------------------------------------------------
top20 <- word_space %>%          #자주 사용되는 단어 추출(상위 20개만)
 head(20)

top20

# ------------------------------------------------------------------------
#그림그리기
library(ggplot2)                                          
                                                          # fill=n을 해야 색이 나옴
ggplot(top20, aes(x = reorder(word, n), y = n,fill=n)) +  # 단어 빈도순 정렬
 geom_col() +                                       
 coord_flip()+                                            # 회전                                                     
 scale_fill_gradient(low="red",high="gold")              #reorder로 정렬해서 그릴 수 있다(정렬하지 않으면 섞여서, 정렬하면 많은거부터)
     

# ------------------------------------------------------------------------
ggplot(top20, aes(x = reorder(word, n), y = n)) +
 geom_col() +
 coord_flip() +
 geom_text(aes(label = n), hjust = -0.3) +            # 막대 밖 빈도 표시
  
 labs(title = "문재인 대통령 출마 연설문 단어 빈도",  # 그래프 제목
      x = NULL, y = NULL) +                           # 축 이름 삭제
  
 theme(title = element_text(size = 12))               # 제목 크기

# ------------------------------------------------------------------------
#워드 클라우드 만들기
#ggplot으로 그려짐
library(ggwordcloud)

ggplot(word_space, aes(label = word, size = n)) +
 geom_text_wordcloud(seed = 1234) +     
 scale_radius(limits = c(3, NA),     # limits 최소, 최대 단어 빈도
              range = c(3, 30))      # range 최소, 최대 글자 크기


# ------------------------------------------------------------------------
ggplot(word_space, 
       aes(label = word, 
           size = n, 
           col = n)) +                     # 빈도에 따라 색깔 표현
 geom_text_wordcloud(seed = 1234) +  
 scale_radius(limits = c(3, NA),
              range = c(3, 30)) +
 scale_color_gradient(low = "#66aaf2",     # 최소 빈도 색깔
                      high = "#004EA1") +  # 최고 빈도 색깔
 theme_minimal()                           # 배경 없는 테마 적용


# ------------------------------------------------------------------------
library(showtext)
library(extrafont)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
font_import(paths=NULL, recursive = TRUE, prompt=TRUE, pattern=NULL)
showtext_auto()

# ------------------------------------------------------------------------
#폰트 
font_add_google(name = "Black Han Sans", family = "blackhansans")
showtext_auto()
ggplot(word_space,
       aes(label = word,
           size = n,
           col = n)) +
  geom_text_wordcloud(seed = 1234,
                      family = "blackhansans") +  # 폰트 적용  #family에 원하는 폰트 넣으면 폰트 바뀜(font.google.com에서 폰트 찾기 가능)
  scale_radius(limits = c(3, NA),
               range = c(3, 30)) +
  scale_color_gradient(low = "#66aaf2",
                       high = "#004EA1") +
  theme_minimal()


# ------------------------------------------------------------------------
font_add_google(name = "Gamja Flower", family = "gamjaflower")
showtext_auto()
ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  
  labs(title = "문재인 대통령 출마 연설문 단어 빈도",
       x = NULL, y = NULL) +
  
  theme(title = element_text(size = 12),
        text = element_text(family = "gamjaflower"))  # 폰트 적용





###예시-------------------------------------------------
library(stringr)
library(dplyr)
library(tidytext)
library(ggplot2)
yoon <- readLines("clipboard")
yoon2 <- yoon %>%
  str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기
  str_squish() %>%                      # 연속된 공백 제거
  as_tibble()  
yoon2 %>%
  unnest_tokens(input = value,
                output = word,
                token = "words") -> daegu3
daegu3 %>% 
  count(word, sort=T) %>% 
  filter(str_count(word)>1) -> daegu4
library(ggplot2)
daegu4 %>% 
  head(20) %>% 
  ggplot(aes(x=word, y=n, fill=n))+geom_col()+coord_flip()
daegu4 %>% 
  head(20) %>% 
  ggplot(aes(reorder(x=word,n), y=n, fill=n))+geom_col()+coord_flip()
library(ggwordcloud)
ggplot(daegu4, aes(label = word, size = n)) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(5, NA),     # 최소, 최대 단어 빈도
               range = c(3, 30)) 
##------------------------------------------------------------------------

# 02-1 --------------------------------------------------------------------

install.packages("multilinguer")
library(multilinguer)
install_jdk()

install.packages(c("stringr", "hash", "tau", "Sejong", "RSQLite", "devtools"),
                 type = "binary")

install.packages("remotes")
remotes::install_github("haven-jeon/KoNLP",
                        upgrade = "never",
                        INSTALL_opts = c("--no-multiarch"))

library(KoNLP)

useNIADic()



# -------------------------------------------------------------------------
library(dplyr)
text <- tibble(
  value = c("대한민국은 민주공화국이다.",
            "대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다."))
text

extractNoun(text$value)

# -------------------------------------------------------------------------
library(tidytext)
text %>%
  unnest_tokens(input = value,        # 분석 대상
                output = word,        # 출력 변수명
                token = extractNoun)  # 토큰화 함수


# -------------------------------------------------------------------------
# 문재인 대통령 연설문 불러오기
raw_moon <- readLines("speech_moon.txt", encoding = "UTF-8")

# 기본적인 전처리
library(stringr)

moon <- raw_moon %>%
  str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기
  str_squish() %>%                      # 중복 공백 제거
  as_tibble()                           # tibble로 변환

# 명사 기준 토큰화
word_noun <- moon %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

word_noun


# 02-2 --------------------------------------------------------------------

word_noun <- word_noun %>%
  count(word, sort = T) %>%    # 단어 빈도 구해 내림차순 정렬
  filter(str_count(word) > 1)  # 두 글자 이상만 남기기

word_noun


# -------------------------------------------------------------------------
# 상위 20개 단어 추출
top20 <- word_noun %>%
  head(20)

# 막대 그래프 만들기
library(ggplot2)
ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))


# -------------------------------------------------------------------------
# 폰트 설정
library(showtext)
font_add_google(name = "Black Han Sans", family = "blackhansans")
showtext_auto()

library(ggwordcloud)
ggplot(word_noun, aes(label = word, size = n, col = n)) +
  geom_text_wordcloud(seed = 1234, family = "blackhansans") +
  scale_radius(limits = c(3, NA),
               range = c(3, 15)) +
  scale_color_gradient(low = "#66aaf2", high = "#004EA1") +
  theme_minimal()


# 02-3 --------------------------------------------------------------------

sentences_moon <- raw_moon %>%
  str_squish() %>%
  as_tibble() %>%
  unnest_tokens(input = value,
                output = sentence,
                token = "sentences")

sentences_moon


# -------------------------------------------------------------------------
str_detect("치킨은 맛있다", "치킨")
str_detect("치킨은 맛있다", "피자")


# -------------------------------------------------------------------------
sentences_moon %>%
  filter(str_detect(sentence, "국민"))


# -------------------------------------------------------------------------
sentences_moon %>%
  filter(str_detect(sentence, "일자리"))

# 03-1 --------------------------------------------------------------------

library(dplyr)

# 문재인 대통령 연설문 불러오기
raw_moon <- readLines("C:/Users/OWNER/Downloads/speech_moon.txt", encoding = "UTF-8")
moon <- raw_moon %>%
  as_tibble() %>%
  mutate(president = "moon")

# 박근혜 대통령 연설문 불러오기
raw_park <- readLines("C:/Users/OWNER/Downloads/speech_park.txt", encoding = "UTF-8")
park <- raw_park %>%
  as_tibble() %>%
  mutate(president = "park")


# -------------------------------------------------------------------------
bind_speeches <- bind_rows(moon, park) %>%
  select(president, value)

head(bind_speeches)
tail(bind_speeches)


# -------------------------------------------------------------------------
# 기본적인 전처리
library(stringr)
speeches <- bind_speeches %>%
  mutate(value = str_replace_all(value, "[^가-힣]", " "),
         value = str_squish(value))

speeches


# 토큰화
library(tidytext)
library(KoNLP)

speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)
speeches



# -------------------------------------------------------------------------
frequency <- speeches %>%
  count(president, word) %>%   # 연설문 및 단어별 빈도
  filter(str_count(word) > 1)  # 두 글자 이상 추출

head(frequency)



# -------------------------------------------------------------------------
top10 <- frequency %>%
  group_by(president) %>%  # president별로 분리
  slice_max(n, n = 10)     # 상위 10개 추출

top10

top10 <- frequency %>%
  group_by(president) %>%  
  slice_max(n, n = 10, with_ties = F) ## 무조거 10개로 짜름
#top10 %>% view

# -------------------------------------------------------------------------
top10 %>%
  filter(president == "park")


# -------------------------------------------------------------------------
df <- tibble(x = c("A", "B", "C", "D"), y = c(4, 3, 2, 2))

df %>% 
  slice_max(y, n = 3)

df %>% 
  slice_max(y, n = 3, with_ties = F)


# -------------------------------------------------------------------------
top10 <- frequency %>%
  group_by(president) %>%
  slice_max(n, n = 10, with_ties = F)

top10


# 그리기   -------------------------------------------------------------
library(ggplot2)
ggplot(top10, aes(x = reorder(word, n),    ##reorder 정렬
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +    ##막대가 누워짐
  facet_wrap(~ president)  ##따로 그려줌


# -------------------------------------------------------------------------
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president,         # president별 그래프 생성
              scales = "free_y")  # y축 통일하지 않음(이빠진그림 이 채워짐)


# -------------------------------------------------------------------------
top10 <- frequency %>%                      ##대통령별로 상위 10개 뽑기
  filter(word != "국민") %>%
  group_by(president) %>%
  slice_max(n, n = 10, with_ties = F)

top10


# -------------------------------------------------------------------------
ggplot(top10, aes(x = reorder(word, n),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y")


# -------------------------------------------------------------------------
ggplot(top10, aes(x = reorder_within(word, n, president),    ##reorder_within대통령 별로 정렬해서 
                  y = n,
                  fill = president)) +                           
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y")


# -------------------------------------------------------------------------
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y") +
  scale_x_reordered() +                               ##scale_x_reordered() 단어뒤에 pack,moon사라지고 단어만 남음
  labs(x = NULL) +                                    # x축 삭제
  theme(text = element_text(family = "nanumgothic"))  # 폰트


# 03-2 --------------------------------------------------------------------

df_long <- frequency %>%
  group_by(president) %>%
  slice_max(n, n = 10) %>%
  filter(word %in% c("국민", "우리", "정치", "행복"))

df_long


# -------------------------------------------------------------------------
if(!require(tidyr)) install.packages("tidyr")
library(tidyr)


df_wide <- df_long %>%
  pivot_wider(names_from = president,
              values_from = n)

df_wide


# -------------------------------------------------------------------------
df_wide <- df_long %>%
  pivot_wider(names_from = president,
              values_from = n,
              values_fill = list(n = 0))

df_wide


# -------------------------------------------------------------------------
frequency_wide <- frequency %>%
  pivot_wider(names_from = president,
              values_from = n,
              values_fill = list(n = 0))

frequency_wide


# -------------------------------------------------------------------------
frequency_wide <- frequency_wide %>%
  mutate(ratio_moon = ((moon + 1)/(sum(moon + 1))),  # moon에서 단어의 비중
         ratio_park = ((park + 1)/(sum(park + 1))))  # park에서 단어의 비중

frequency_wide


# -------------------------------------------------------------------------
frequency_wide <- frequency_wide %>%
  mutate(odds_ratio = ratio_moon/ratio_park)

frequency_wide %>%
  arrange(-odds_ratio)

frequency_wide %>%
  arrange(odds_ratio)


# -------------------------------------------------------------------------
frequency_wide <- frequency_wide %>%
  mutate(ratio_moon  = ((moon + 1)/(sum(moon + 1))),
         ratio_park  = ((park + 1)/(sum(park + 1))),
         odds_ratio = ratio_moon/ratio_park)


# -------------------------------------------------------------------------
frequency_wide <- frequency_wide %>%
  mutate(odds_ratio = ((moon + 1)/(sum(moon + 1)))/
                      ((park + 1)/(sum(park + 1))))


# -------------------------------------------------------------------------
top10 <- frequency_wide %>%
  filter(rank(odds_ratio) <= 10 | rank(-odds_ratio) <= 10)

top10 %>%
  arrange(-odds_ratio)


# -------------------------------------------------------------------------
df <- tibble(x = c(2, 5, 10))
df %>% mutate(y = rank(x))     # 값이 작을수록 앞순위
df %>% mutate(y = rank(-x))    # 값이 클수록 앞순위


# -------------------------------------------------------------------------
top10 <- top10 %>%
  mutate(president = ifelse(odds_ratio > 1, "moon", "park"),
         n = ifelse(odds_ratio > 1, moon, park))

top10


# -------------------------------------------------------------------------
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free_y") +
  scale_x_reordered()


# -------------------------------------------------------------------------
ggplot(top10, aes(x = reorder_within(word, n, president),
                  y = n,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~ president, scales = "free") +
  scale_x_reordered() +
  labs(x = NULL) +                                    # x축 삭제
  theme(text = element_text(family = "nanumgothic"))  # 폰트


# -------------------------------------------------------------------------
speeches_sentence <- bind_speeches %>%
  as_tibble() %>%
  unnest_tokens(input = value,
                output = sentence,
                token = "sentences")

head(speeches_sentence)
tail(speeches_sentence)


# -------------------------------------------------------------------------
speeches_sentence %>%
  filter(president == "moon" & str_detect(sentence, "복지국가"))


# -------------------------------------------------------------------------
speeches_sentence %>%
  filter(president == "park" & str_detect(sentence, "행복"))


# -------------------------------------------------------------------------
frequency_wide %>%
  arrange(abs(1 - odds_ratio)) %>%
  head(10)


# -------------------------------------------------------------------------
frequency_wide %>%
  filter(moon >= 5 & park >= 5) %>%
  arrange(abs(1 - odds_ratio)) %>%
  head(10)


# 03-3 --------------------------------------------------------------------

frequency_wide <- frequency_wide %>%
  mutate(log_odds_ratio = log(odds_ratio))


# -------------------------------------------------------------------------
# moon에서 비중이 큰 단어
frequency_wide %>%
  arrange(-log_odds_ratio)

# park에서 비중이 큰 단어
frequency_wide %>%
  arrange(log_odds_ratio)

# 비중이 비슷한 단어
frequency_wide %>%
  arrange(abs(log_odds_ratio))


# -------------------------------------------------------------------------
frequency_wide <- frequency_wide %>%
  mutate(log_odds_ratio = log(((moon + 1) / (sum(moon + 1))) /
                              ((park + 1) / (sum(park + 1)))))


# -------------------------------------------------------------------------
top10 <- frequency_wide %>%
  group_by(president = ifelse(log_odds_ratio > 0, "moon", "park")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)

top10 %>% 
  arrange(-log_odds_ratio) %>% 
  select(word, log_odds_ratio, president)


# -------------------------------------------------------------------------
ggplot(top10, aes(x = reorder(word, log_odds_ratio),
                  y = log_odds_ratio,
                  fill = president)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))


# 03-4 --------------------------------------------------------------------

# 데이터 불러오기
library(readr)

raw_speeches <- read_csv("speeches_presidents.csv")
raw_speeches


# -------------------------------------------------------------------------
# 기본적인 전처리
speeches <- raw_speeches %>%
  mutate(value = str_replace_all(value, "[^가-힣]", " "),
         value = str_squish(value))

# 토큰화
speeches <- speeches %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

# 단어 빈도 구하기
frequency <- speeches %>%
  count(president, word) %>%
  filter(str_count(word) > 1)

frequency


# -------------------------------------------------------------------------
frequency <- frequency %>%
  bind_tf_idf(term = word,           # 단어
              document = president,  # 텍스트 구분 변수
              n = n) %>%             # 단어 빈도
  arrange(-tf_idf)

frequency


# -------------------------------------------------------------------------
frequency %>% filter(president == "문재인")

frequency %>% filter(president == "박근혜")

frequency %>% filter(president == "이명박")

frequency %>% filter(president == "노무현")


# -------------------------------------------------------------------------
frequency %>%
  filter(president == "문재인") %>%
  arrange(tf_idf)

frequency %>%
  filter(president == "박근혜") %>%
  arrange(tf_idf)


# -------------------------------------------------------------------------
# 주요 단어 추출
top10 <- frequency %>%
  group_by(president) %>%
  slice_max(tf_idf, n = 10, with_ties = F)

# 그래프 순서 정하기
top10$president <- factor(top10$president,
                          levels = c("문재인", "박근혜", "이명박", "노무현"))

# 막대 그래프 만들기
ggplot(top10, aes(x = reorder_within(word, tf_idf, president),
                  y = tf_idf,
                  fill = president)) +  
  geom_col(show.legend = F) +
  coord_flip() +
  facet_wrap(~ president, scales = "free", ncol = 2) +
  scale_x_reordered() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))

----------------------------------------------------------------------과제
library(jsonlite)
library(dplyr)
petitions = read_json("petitions_2019-08.json", simplifyVector = T)
head(petitions)
head(news)

library(multilinguer)
library(stringr)

#1. 분류별 청원의 수는?
library(dplyr)
petitions %>% group_by(category) %>% summarise(n=n())->a

#2. "보건복지"에서 많이 언급된 키워드 상위 15개는?
library(tidytext)
library(KoNLP)

petitions %>% 
  filter(category=="보건복지") %>%  
  mutate(content=str_replace_all(content, "[^가-힣]", " "),
         content=str_squish(content)) %>%  
  unnest_tokens(input = content,
                output = word,
                token = extractNoun)->b

b %>% count(word,sort=T) %>% 
  filter(str_count(word)>1)->b2
head(b2,15)

#3. "보건복지"에 대한 워드클라우드를 그리세요.
library(wordcloud2)
wordcloud2(b2)


# 4. "보건복지"와 "정치개혁"을 비교했을 때 보건복지에서 더 많이 사용된 단어 15개,
petitions %>%
  filter(category=="보건복지"|category=="정치개혁") %>%
  mutate(content = str_replace_all(content, "[^가-？]", " "),
         content = str_squish(content)) %>%
  unnest_tokens(input = content,
                output = word,
                token = extractNoun) -> out3
out3 %>% count(category, word, sort=T) %>%
  filter(str_count(word)>1) -> out4

library(tidyr)
out4 %>% pivot_wider(id_cols=word, names_from=category,
                     values_from=n, values_fill=0) -> out5
out5 %>%
  mutate(ratio1=(보건복지+1)/(sum(보건복지)+1)) %>%
  mutate(ratio2=(정치개혁+1)/(sum(정치개혁)+1)) %>%
  mutate(odds = ratio1/ratio2) %>%
  mutate(logodds=log(odds)) %>%
  slice_max(logodds, n=10)

out5 %>%
  mutate(ratio1=(보건복지+1)/(sum(보건복지)+1)) %>%
  mutate(ratio2=(정치개혁+1)/(sum(정치개혁)+1)) %>%
  mutate(odds = ratio1/ratio2) %>%
  mutate(logodds=log(odds)) %>%
  slice_max(-logodds, n=15)


head(news)
table(news$class)
u.news= unique(news$class)

news %>%
  filter(class==u.news[1]) %>%
  mutate(text = str_replace_all(text, "[^가-？]", " "),
         text = str_squish(text)) %>%
  unnest_tokens(input = text,
                output = word,
                token = extractNoun) -> out1
out1 %>% count(word, sort=T) %>%
  filter(str_count(word)>1) -> out2
wordcloud2(out2, minSize=5)

# 6. TF-IDF를 이용하여 뉴스주제별로 상대적으로 많이 사용된 키워드가 무엇인지 확인하세요.
news %>%
  mutate(text = str_replace_all(text, "[^가-？]", " "),
         text = str_squish(text)) %>%
  unnest_tokens(input = text,
                output = word,
                token = extractNoun) -> out3
out3 %>% count(class, word, sort=T) %>%
  filter(str_count(word)>1) -> out4

out4 %>%
  bind_tf_idf(term = word, # 단어
              document = class, # 텍스트 구분 변수
              n = n) %>% # 단어 빈도
  group_by(class) %>%
  slice_max(tf_idf,n=5, with_ties = F) %>% View


# 04-1 --------------------------------------------------------------------

# 감정 사전 불러오기
library(readr)
dic <- read_csv("C:/Users/OWNER/Documents/knu_sentiment_lexicon.csv")



# -------------------------------------------------------------------------
library(dplyr)

# 긍정 단어
dic %>% 
  filter(polarity == 2) %>%     #polarity == 1 (1정도에 해당되는 것들 보여줌)
  arrange(word)                 #음수 부정 양수 긍정

# 부정 단어
dic %>% 
  filter(polarity == -2) %>% 
  arrange(word)


# -------------------------------------------------------------------------
dic %>% 
  filter(word %in% c("좋은", "나쁜"))

dic %>% 
  filter(word %in% c("기쁜", "슬픈"))

dic %>%
  filter(word %in% c("행복하다", "좌절하다"))


# -------------------------------------------------------------------------
# 이모티콘
library(stringr)
dic %>% 
  filter(!str_detect(word, "[가-힣]")) %>% 
  arrange(word)


# -------------------------------------------------------------------------
dic %>% 
  mutate(sentiment = ifelse(polarity >=  1, "pos",
                     ifelse(polarity <= -1, "neg", "neu"))) %>% 
  count(sentiment)


#문장의 감정 점수 구하기
# -------------------------------------------------------------------------
df <- tibble(sentence = c("디자인 예쁘고 마감도 좋아서 만족스럽다.",
                          "디자인은 괜찮다. 그런데 마감이 나쁘고 가격도 비싸다."))
df

library(tidytext)
df <- df %>% 
  unnest_tokens(input = sentence,
                output = word,
                token = "words",
                drop = F)

df


# -------------------------------------------------------------------------
df <- df %>% 
  left_join(dic, by = "word") %>% 
  mutate(polarity = ifelse(is.na(polarity), 0, polarity))

df


# -------------------------------------------------------------------------
score_df <- df %>% 
  group_by(sentence) %>% 
  summarise(score  = sum(polarity))

score_df


# 04-2 --------------------------------------------------------------------

# 데이터 불러오기
raw_news_comment <- read_csv("C:/Users/OWNER/Downloads/news_comment_parasite.csv")


# -------------------------------------------------------------------------
# 기본적인 전처리
install.packages("textclean")
library(textclean)

news_comment <- raw_news_comment %>%
  mutate(id = row_number(),
         reply = str_squish(replace_html(reply)))

# 데이터 구조 확인
glimpse(news_comment)


# -------------------------------------------------------------------------
# 토큰화
word_comment <- news_comment %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words",
                drop = F)

word_comment %>%
  select(word, reply)

# 감정 점수 부여
word_comment <- word_comment %>%
  left_join(dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity))

word_comment %>%
  select(word, polarity)


# -------------------------------------------------------------------------
word_comment <- word_comment %>%
  mutate(sentiment = ifelse(polarity ==  2, "pos",
                     ifelse(polarity == -2, "neg", "neu")))

word_comment %>%
  count(sentiment)


# -------------------------------------------------------------------------
top10_sentiment <- word_comment %>%
  filter(sentiment != "neu") %>%
  count(sentiment, word) %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10)

top10_sentiment


# -------------------------------------------------------------------------
# 막대 그래프 만들기
library(ggplot2)
ggplot(top10_sentiment, aes(x = reorder(word, n), 
                            y = n, 
                            fill = sentiment)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  facet_wrap(~ sentiment, scales = "free") +
  scale_y_continuous(expand = expansion(mult = c(0.05, 0.15))) +  
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))


# -------------------------------------------------------------------------
score_comment <- word_comment %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

score_comment %>% 
  select(score, reply)


# -------------------------------------------------------------------------
# 긍정 댓글
score_comment %>%
  select(score, reply) %>% 
  arrange(-score)

# 부정 댓글
score_comment %>%
  select(score, reply) %>% 
  arrange(score)


# -------------------------------------------------------------------------
score_comment %>%
  count(score)


# -------------------------------------------------------------------------
score_comment <- score_comment %>%
  mutate(sentiment = ifelse(score >=  1, "pos",
                     ifelse(score <= -1, "neg", "neu")))


# -------------------------------------------------------------------------
frequency_score <- score_comment %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100)

frequency_score


# -------------------------------------------------------------------------
# 막대 그래프 만들기
ggplot(frequency_score, aes(x = sentiment, y = n, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3) + 
  scale_x_discrete(limits = c("pos", "neu", "neg"))


# -------------------------------------------------------------------------
df <- tibble(contry = c("Korea", "Korea", "Japen", "Japen"),  # 축
             sex = c("M", "F", "M", "F"),                     # 누적 막대
             ratio = c(60, 40, 30, 70))                       # 값
df

ggplot(df, aes(x = contry, y = ratio, fill = sex)) + geom_col()



# -------------------------------------------------------------------------
ggplot(df, aes(x = contry, y = ratio, fill = sex)) + 
  geom_col() +
  geom_text(aes(label = paste0(ratio, "%")),          # % 표시
            position = position_stack(vjust = 0.5))   # 가운데 표시


# -------------------------------------------------------------------------
# 더미 변수 생성
frequency_score$dummy <- 0
frequency_score


# -------------------------------------------------------------------------
ggplot(frequency_score, aes(x = dummy, y = ratio, fill = sentiment)) +
  geom_col() +
  geom_text(aes(label = paste0(round(ratio, 1), "%")),      
              position = position_stack(vjust = 0.5)) + 
  theme(axis.title.x = element_blank(),  # x축 이름 삭제
        axis.text.x  = element_blank(),  # x축 값 삭제
        axis.ticks.x = element_blank())  # x축 눈금 삭제


# 04-3 --------------------------------------------------------------------

comment <- score_comment %>%
  unnest_tokens(input = reply,          # 단어 기준 토큰화
                output = word,
                token = "words",
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") &  # 한글 추출
         str_count(word) >= 2)          # 두 글자 이상 추출


# -------------------------------------------------------------------------
# 감정 및 단어별 빈도 구하기
frequency_word <- comment %>%
  filter(str_count(word) >= 2) %>%
  count(sentiment, word, sort = T)


# -------------------------------------------------------------------------
# 긍정 댓글 고빈도 단어
frequency_word %>%
  filter(sentiment == "pos")

# 부정 댓글 고빈도 단어
frequency_word %>%
  filter(sentiment == "neg")


# -------------------------------------------------------------------------
library(tidyr)
comment_wide <- frequency_word %>%
  filter(sentiment != "neu") %>%  # 중립 제외
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = list(n = 0))

comment_wide


# -------------------------------------------------------------------------
# 로그 오즈비 구하기
comment_wide <- comment_wide %>%
  mutate(log_odds_ratio = log(((pos + 1) / (sum(pos + 1))) /
                              ((neg + 1) / (sum(neg + 1)))))

comment_wide


# -------------------------------------------------------------------------
top10 <- comment_wide %>%
  group_by(sentiment = ifelse(log_odds_ratio > 0, "pos", "neg")) %>%
  slice_max(abs(log_odds_ratio), n = 10)

top10 %>% print(n = Inf)


# -------------------------------------------------------------------------
top10 <- comment_wide %>%
  group_by(sentiment = ifelse(log_odds_ratio > 0, "pos", "neg")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)

top10 


# -------------------------------------------------------------------------
# 막대 그래프 만들기
ggplot(top10, aes(x = reorder(word, log_odds_ratio),
                      y = log_odds_ratio,
                      fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))


# 04-4 --------------------------------------------------------------------

# "소름"이 사용된 댓글
score_comment %>%
  filter(str_detect(reply, "소름")) %>%
  select(reply)

# "미친"이 사용된 댓글
score_comment %>%
  filter(str_detect(reply, "미친")) %>%
  select(reply)


# -------------------------------------------------------------------------
dic %>% filter(word %in% c("소름", "소름이", "미친"))


# -------------------------------------------------------------------------
new_dic <- dic %>%
  mutate(polarity = ifelse(word %in% c("소름", "소름이", "미친"), 2, polarity))

new_dic %>% filter(word %in% c("소름", "소름이", "미친"))


# -------------------------------------------------------------------------
new_word_comment <- word_comment %>%
  select(-polarity) %>%
  left_join(new_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity))


# -------------------------------------------------------------------------
new_score_comment <- new_word_comment %>%
  group_by(id, reply) %>%
  summarise(score = sum(polarity)) %>%
  ungroup()

new_score_comment %>%
  select(score, reply) %>%
  arrange(-score)


# -------------------------------------------------------------------------
# 1점 기준으로 긍정 중립 부정 분류
new_score_comment <- new_score_comment %>%
  mutate(sentiment = ifelse(score >=  1, "pos",
                     ifelse(score <= -1, "neg", "neu")))


# -------------------------------------------------------------------------
# 원본 감정 사전 활용
score_comment %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100)

# 수정한 감정 사전 활용
new_score_comment %>%
  count(sentiment) %>%
  mutate(ratio = n/sum(n)*100)


# -------------------------------------------------------------------------
word <- "소름|소름이|미친"

# 원본 감정 사전 활용
score_comment %>%
  filter(str_detect(reply, word)) %>%
  count(sentiment)

# 수정한 감정 사전 활용
new_score_comment %>%
  filter(str_detect(reply, word)) %>%
  count(sentiment)


# -------------------------------------------------------------------------
df <- tibble(sentence = c("이번 에피소드 쩐다", 
                          "이 영화 핵노잼")) %>% 
  unnest_tokens(input = sentence, 
                output = word, 
                token = "words", 
                drop = F)

df %>% 
  left_join(dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity)) %>% 
  group_by(sentence) %>% 
  summarise(score = sum(polarity))


# -------------------------------------------------------------------------
# 신조어 목록 생성
newword <- tibble(word = c("쩐다", "핵노잼"), 
                  polarity = c(2, -2))

# 사전에 신조어 추가
newword_dic <- bind_rows(dic, newword)

# 새 사전으로 감정 점수 부여
df %>% 
  left_join(newword_dic, by = "word") %>%
  mutate(polarity = ifelse(is.na(polarity), 0, polarity)) %>% 
  group_by(sentence) %>% 
  summarise(score = sum(polarity))


# -------------------------------------------------------------------------
# 토큰화 및 전처리
new_comment <- new_score_comment %>%
  unnest_tokens(input = reply,
                output = word,
                token = "words",
                drop = F) %>%
  filter(str_detect(word, "[가-힣]") &
           str_count(word) >= 2)

# 감정 및 단어별 빈도 구하기
new_frequency_word <- new_comment %>%
  count(sentiment, word, sort = T)


# -------------------------------------------------------------------------
# Wide form으로 변환
new_comment_wide <- new_frequency_word %>%
  filter(sentiment != "neu") %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = list(n = 0))

# 로그 오즈비 구하기
new_comment_wide <- new_comment_wide %>%
  mutate(log_odds_ratio = log(((pos + 1) / (sum(pos + 1))) /
                              ((neg + 1) / (sum(neg + 1)))))


# -------------------------------------------------------------------------
new_top10 <- new_comment_wide %>%
  group_by(sentiment = ifelse(log_odds_ratio > 0, "pos", "neg")) %>%
  slice_max(abs(log_odds_ratio), n = 10, with_ties = F)

# 막대 그래프 만들기
ggplot(new_top10, aes(x = reorder(word, log_odds_ratio),
                      y = log_odds_ratio,
                      fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))


# -------------------------------------------------------------------------
# 원본 감정 사전 활용
top10 %>% 
  select(-pos, -neg) %>% 
  arrange(-log_odds_ratio)

# 수정한 감정 사전 활용
new_top10 %>%
  select(-pos, -neg) %>%
  arrange(-log_odds_ratio)


# -------------------------------------------------------------------------
new_comment_wide %>%
  filter(word == "미친")


# -------------------------------------------------------------------------
# 긍정 댓글 원문
new_score_comment %>%
  filter(sentiment == "pos" & str_detect(reply, "축하")) %>%
  select(reply)

new_score_comment %>%
  filter(sentiment == "pos" & str_detect(reply, "소름")) %>%
  select(reply)


# -------------------------------------------------------------------------
# 부정 댓글 원문
new_score_comment %>%
  filter(sentiment == "neg" & str_detect(reply, "좌빨")) %>%
  select(reply)

new_score_comment %>%
  filter(sentiment == "neg" & str_detect(reply, "못한")) %>%
  select(reply)


# 05-1 --------------------------------------------------------------------

# 기생충 기사 댓글 불러오기
library(readr)
raw_news_comment <- read_csv("news_comment_parasite (1).csv")


# 전처리
library(dplyr)
library(stringr)
library(textclean)

news_comment <- raw_news_comment %>%
  select(reply) %>%
  mutate(reply = str_replace_all(reply, "[^가-힣]", " "),
         reply = str_squish(reply),
         id = row_number())


# -------------------------------------------------------------------------
# 형태소 분석하기
library(tidytext)
library(KoNLP)

comment_pos <- news_comment %>%
  unnest_tokens(input = reply,
                output = word,
                token = SimplePos22,
                drop = F)

comment_pos %>% 
  select(word, reply)


# -------------------------------------------------------------------------
# 품사별로 행 분리
library(tidyr)
comment_pos <- comment_pos %>%
  separate_rows(word, sep = "[+]")  # +가 있으면 밑으로 내려줘

comment_pos %>% 
  select(word, reply)


# -------------------------------------------------------------------------
# 명사 추출하기
noun <- comment_pos %>%
  filter(str_detect(word, "/n")) %>%   # n다음에 있는거 지우기
  mutate(word = str_remove(word, "/.*$"))  # *아무숫자

noun %>%
  select(word, reply)


# -------------------------------------------------------------------------
noun %>%
  count(word, sort = T)


# -------------------------------------------------------------------------
# 동사, 형용사 추출하기
pvpa <- comment_pos %>%
  filter(str_detect(word, "/pv|/pa")) %>%         # "/pv", "/pa" 추출(pv동사 pa형용사)/|합집합
  mutate(word = str_replace(word, "/.*$", "다"))  # "/"로 시작 문자를 "다"로 바꾸기

pvpa %>%
  select(word, reply)


# -------------------------------------------------------------------------
pvpa %>%
  count(word, sort = T)


# -------------------------------------------------------------------------
# 품사 결합
comment <- bind_rows(noun, pvpa) %>% #noun명사만저장,pvpa동사형용사만
  filter(str_count(word) >= 2) %>%
  arrange(id)

comment %>%
  select(word, reply)


# -------------------------------------------------------------------------
#명사, 동사, 형용사를 한번에 추출하기
#명사, 동사, 형용사를 추출해 결합한 후 두 글자 이상만 남기기
comment_new <- comment_pos %>%
  separate_rows(word, sep = "[+]") %>%
  filter(str_detect(word, "/n|/pv|/pa")) %>%
  mutate(word = ifelse(str_detect(word, "/pv|/pa"),
                       str_replace(word, "/.*$", "다"), #동사 형용사 뒤에 다 붙이기
                       str_remove(word, "/.*$"))) %>% #명사 뒤에 지우기
  filter(str_count(word) >= 2) %>%
  arrange(id)


# -------------------------------------------------------------------------
#단어 동시 출현 빈도 구하기
install.packages("widyr")
library(widyr)

pair <- comment %>%
  pairwise_count(item = word,
                 feature = id,
                 sort = T)
pair
#대응되는거 항상 두개씩 나온다

# -------------------------------------------------------------------------
#특정 단어와 자주 함께 사용된 단어 살펴보기
pair %>% filter(item1 == "영화")

pair %>% filter(item1 == "봉준호")


# 05-2 --------------------------------------------------------------------

install.packages("tidygraph")
library(tidygraph)

graph_comment <- pair %>%  #빈도수가 큰거만 그리기
  filter(n >= 25) %>%     #n >= 25 숫자 너무 작게하면 복잡해짐
  as_tbl_graph()        #as_tbl_graph()로 바꿔둬야 그래프 그려짐

graph_comment


# -------------------------------------------------------------------------
install.packages("ggraph")
library(ggraph)

ggraph(graph_comment) +
  geom_edge_link() +                 # 엣지
  geom_node_point() +                # 노드
  geom_node_text(aes(label = name))  # 텍스트


# -------------------------------------------------------------------------
library(showtext)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
showtext_auto()


# -------------------------------------------------------------------------
#그림 좀 더 이쁘게 그리기
set.seed(1234)                              # 난수 고정
ggraph(graph_comment, layout = "fr") +      # 레이아웃(모양) (helep에서 ggraph치면 포함된 레이아웃들 나옴/구글 검색)
  
  geom_edge_link(color = "gray50",          # 엣지 색깔
                 alpha = 0.5,               # 엣지 명암
                 width=2) +                #선 굵기 지정 가능(굳이 필요 없음 기본값으로 해도 잘 나옴)

  geom_node_point(color = "lightcoral",     # 노드 색깔
                  size = 5) +               # 노드 크기

  geom_node_text(aes(label = name),         # 텍스트 표시
                 repel = T,                 # 노드밖 표시
                 size = 5,                  # 텍스트 크기
                 family = "nanumgothic") +  # 폰트

  theme_graph()                             # 배경 삭제


# -------------------------------------------------------------------------
#네트워크 그래프 함수 만들다
word_network <- function(x) {
  ggraph(x, layout = "fr") +
    geom_edge_link(color = "gray50",
                   alpha = 0.5) +
    geom_node_point(color = "lightcoral",
                    size = 5) +
    geom_node_text(aes(label = name),
                   repel = T,
                   size = 5,
                   family = "nanumgothic") +
    theme_graph()
}


# -------------------------------------------------------------------------
set.seed(1234)
word_network(graph_comment)


# -------------------------------------------------------------------------
# 유의어 처리하기(비슷한 단어 동일하게 처리)
comment <- comment %>%
  mutate(word = ifelse(str_detect(word, "감독") &    #감독은 다 봉준호로
                      !str_detect(word, "감독상"), "봉준호", word), 
         word = ifelse(word == "오르다", "올리다", word),          #오르다는 올리다
         word = ifelse(str_detect(word, "축하"), "축하", word))

# 단어 동시 출현 빈도 구하기
pair <- comment %>%
  pairwise_count(item = word,
                 feature = id,
                 sort = T)

# 네트워크 그래프 데이터 만들기
graph_comment <- pair %>%
  filter(n >= 25) %>%
  as_tbl_graph()

# 네트워크 그래프 만들기
set.seed(1234)
word_network(graph_comment)


# -------------------------------------------------------------------------
#네트워크 그래프 데이터에 연결 중심성, 커뮤니티 변수 추가하기
set.seed(1234)
graph_comment <- pair %>%
  filter(n >= 25) %>%
  as_tbl_graph(directed = F) %>%
  mutate(centrality = centrality_degree(),        # 연결 중심성(얼마만큼 겹쳐있는지)
         group = as.factor(group_infomap()))      # 커뮤니티

graph_comment


# -------------------------------------------------------------------------
#네트워크 그래프 데이터에 연결 중심성, 커뮤니티 표현하기
set.seed(1234)
ggraph(graph_comment, layout = "fr") +      # 레이아웃
  
  geom_edge_link(color = "gray50",          # 엣지 색깔
                 alpha = 0.5) +             # 엣지 명암
  
  geom_node_point(aes(size = centrality,    # 노드 크기
                      color = group),       # 노드 색깔
                  show.legend = F) +        # 범례 삭제
  scale_size(range = c(5, 15)) +            # 노드 크기 범위
  
  geom_node_text(aes(label = name),         # 텍스트 표시
                 repel = T,                 # 노드밖 표시
                 size = 5,                  # 텍스트 크기
                 family = "nanumgothic") +  # 폰트
  
  theme_graph()                             # 배경 삭제


# -------------------------------------------------------------------------
#네트워크의 주요 단어 살펴보기
#주요 단어의 커뮤니티 살펴보기
graph_comment %>%
  filter(name == "봉준호")


# -------------------------------------------------------------------------
#같은 커뮤니티로 분류된 단어 살펴보기
graph_comment %>%
  filter(group == 4) %>%
  arrange(-centrality) %>%
  data.frame()

#연결 중심성이 높은 주요 단어 살펴보기
graph_comment %>%
  arrange(-centrality)


# -------------------------------------------------------------------------
#연결 중심성이 높은 주요 단어 살펴보기
#--2번 커뮤니티로 분류된 단어
graph_comment %>%
  filter(group == 2) %>%
  arrange(-centrality) %>%
  data.frame()


# -------------------------------------------------------------------------
#4주요 단어가 사용된 원문 살펴보기

news_comment %>%
  filter(str_detect(reply, "봉준호") & str_detect(reply, "대박")) %>%
  select(reply)

news_comment %>%
  filter(str_detect(reply, "박근혜") & str_detect(reply, "블랙리스트")) %>%
  select(reply)

news_comment %>%
  filter(str_detect(reply, "기생충") & str_detect(reply, "조국")) %>%
  select(reply)


#수업 ㄴㄴ
# 05-3 --------------------------------------------------------------------

word_cors <- comment %>%
  add_count(word) %>%
  filter(n >= 20) %>%
  pairwise_cor(item = word,
               feature = id,
               sort = T)

word_cors


# -------------------------------------------------------------------------
word_cors %>% 
  filter(item1 == "대한민국")

word_cors %>% 
  filter(item1 == "역사")


# -------------------------------------------------------------------------
# 관심 단어 목록 생성
target <- c("대한민국", "역사", "수상소감", "조국", "박근혜", "블랙리스트")

top_cors <- word_cors %>%
  filter(item1 %in% target) %>%
  group_by(item1) %>%
  slice_max(correlation, n = 8)


# -------------------------------------------------------------------------
# 그래프 순서 정하기
top_cors$item1 <- factor(top_cors$item1, levels = target)

library(ggplot2)
ggplot(top_cors, aes(x = reorder_within(item2, correlation, item1),
                 y = correlation,
                 fill = item1)) +
  geom_col(show.legend = F) +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))


# -------------------------------------------------------------------------
set.seed(1234)
graph_cors <- word_cors %>%
  filter(correlation >= 0.15) %>%
  as_tbl_graph(directed = F) %>%
  mutate(centrality = centrality_degree(),
         group = as.factor(group_infomap()))


# -------------------------------------------------------------------------
set.seed(1234)
ggraph(graph_cors, layout = "fr") +

  geom_edge_link(color = "gray50",
                 aes(edge_alpha = correlation,   # 엣지 명암
                     edge_width = correlation),  # 엣지 두께
                 show.legend = F) +              # 범례 삭제
  scale_edge_width(range = c(1, 4)) +            # 엣지 두께 범위
  
  geom_node_point(aes(size = centrality,
                      color = group),
                  show.legend = F) +
  scale_size(range = c(5, 10)) +

  geom_node_text(aes(label = name),
                 repel = T,
                 size = 5,
                 family = "nanumgothic") +

  theme_graph()


# 05-4 --------------------------------------------------------------------

text <- tibble(value = "대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.")

# 바이그램 토큰화
text %>%
  unnest_tokens(input = value,
                output = word,
                token = "ngrams",
                n = 2)

# 트라이그램 토큰화
text %>%
  unnest_tokens(input = value,
                output = word,
                token = "ngrams",
                n = 3)


# -------------------------------------------------------------------------
# 단어 기준 토큰화
text %>%
  unnest_tokens(input = value,
                output = word,
                token = "words")

# 유니그램 토큰화
text %>%
  unnest_tokens(input = value,
                output = word,
                token = "ngrams",
                n = 1)


# -------------------------------------------------------------------------
comment_new <- comment_pos %>%
  separate_rows(word, sep = "[+]") %>%
  filter(str_detect(word, "/n|/pv|/pa")) %>%
  mutate(word = ifelse(str_detect(word, "/pv|/pa"),
                       str_replace(word, "/.*$", "다"),
                       str_remove(word, "/.*$"))) %>%
  filter(str_count(word) >= 2) %>%
  arrange(id)


# -------------------------------------------------------------------------
comment_new <- comment_new %>%
  mutate(word = ifelse(str_detect(word, "감독") &
                      !str_detect(word, "감독상"), "봉준호", word), 
         word = ifelse(word  == "오르다", "올리다", word),
         word = ifelse(str_detect(word, "축하"), "축하", word))

comment_new %>%
  select(word)


# -------------------------------------------------------------------------
line_comment <- comment_new %>%
  group_by(id) %>%
  summarise(sentence = paste(word, collapse = " "))

line_comment


# -------------------------------------------------------------------------
bigram_comment <- line_comment %>%
  unnest_tokens(input = sentence,
                output = bigram,
                token = "ngrams",
                n = 2)

bigram_comment


# -------------------------------------------------------------------------
# 바이그램 분리하기
bigram_seprated <- bigram_comment %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigram_seprated


# -------------------------------------------------------------------------
# 단어쌍 빈도 구하기
pair_bigram <- bigram_seprated %>%
  count(word1, word2, sort = T) %>%
  na.omit()

pair_bigram


# -------------------------------------------------------------------------
# 동시 출현 단어쌍
pair %>%
  filter(item1 == "대한민국")

# 바이그램 단어쌍
pair_bigram %>%
  filter(word1 == "대한민국")

# 동시 출현 단어쌍
pair %>%
  filter(item1 == "아카데미")

# 바이그램 단어쌍
pair_bigram %>%
  filter(word1 == "아카데미")


# -------------------------------------------------------------------------
# 네트워크 그래프 데이터 만들기
graph_bigram <- pair_bigram %>%
  filter(n >= 8) %>%
  as_tbl_graph()

# 네트워크 그래프 만들기
set.seed(1234)
word_network(graph_bigram)


# -------------------------------------------------------------------------
bigram_seprated <- bigram_seprated %>%
  mutate(word1 = ifelse(str_detect(word1, "대단"), "대단", word1),
         word2 = ifelse(str_detect(word2, "대단"), "대단", word2),

         word1 = ifelse(str_detect(word1, "자랑"), "자랑", word1),
         word2 = ifelse(str_detect(word2, "자랑"), "자랑", word2),

         word1 = ifelse(str_detect(word1, "짝짝짝"), "짝짝짝", word1),
         word2 = ifelse(str_detect(word2, "짝짝짝"), "짝짝짝", word2)) %>%

  # 같은 단어 연속 제거
  filter(word1 != word2)

# 단어쌍 빈도 구하기
pair_bigram <- bigram_seprated %>%
  count(word1, word2, sort = T) %>%
  na.omit()


# -------------------------------------------------------------------------
bigram_seprated_new <- bigram_seprated %>%
  mutate_at(vars("word1", "word2"),
            ~ case_when(
              str_detect(., "대단") ~ "대단",
              str_detect(., "자랑") ~ "자랑",
              str_detect(., "짝짝짝") ~ "짝짝짝",
              T ~ .))


# -------------------------------------------------------------------------
# 네트워크 그래프 데이터 만들기
set.seed(1234)
graph_bigram <- pair_bigram %>%
  filter(n >= 8) %>%
  as_tbl_graph(directed = F) %>%
  mutate(centrality = centrality_degree(),    # 중심성
         group = as.factor(group_infomap()))  # 커뮤니티


# -------------------------------------------------------------------------
# 네트워크 그래프 만들기
set.seed(1234)
ggraph(graph_bigram, layout = "fr") +         # 레이아웃
  
  geom_edge_link(color = "gray50",            # 엣지 색깔
                 alpha = 0.5) +               # 엣지 명암
    
  geom_node_point(aes(size = centrality,      # 노드 크기
                      color = group),         # 노드 색깔
                  show.legend = F) +          # 범례 삭제
  scale_size(range = c(4, 8)) +               # 노드 크기 범위
  
  geom_node_text(aes(label = name),           # 텍스트 표시
                 repel = T,                   # 노드밖 표시
                 size = 5,                    # 텍스트 크기
                 family = "nanumgothic") +    # 폰트
  
  theme_graph()                               # 배경 삭제

