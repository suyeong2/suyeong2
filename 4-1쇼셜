# 01-1 -------------------------------------------------------------------

raw_moon <- readLines("C:/Users/OWNER/Downloads/speech_moon.txt", encoding = "UTF-8")
head(raw_moon)

txt <- "치킨은!! 맛있다. xyz 정말 맛있다!@#"
txt

library(stringr)

str_replace_all(string = txt, pattern = "[^가-힣]", replacement = " ")

##[0-9],[a-z],[A-Z],[가-힣]

# ------------------------------------------------------------------------
moon <- raw_moon %>%
 str_replace_all("[^가-힣]", " ")

head(moon)


# ------------------------------------------------------------------------
# 파라미터명 입력
str_replace_all(string = txt, pattern = "[^가-힣]", replacement = " ")

# 파라미터명 생략
str_replace_all(txt, "[^가-힣]", " ")


# ------------------------------------------------------------------------
txt <- "치킨은  맛있다   정말 맛있다  "
txt
str_squish(txt)


# ------------------------------------------------------------------------
moon <- moon %>%
 str_squish()

head(moon)


# ------------------------------------------------------------------------
library(dplyr)
moon <- as_tibble(moon)
moon


# ------------------------------------------------------------------------
moon <- raw_moon %>%
 str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기
 str_squish() %>%                      # 연속된 공백 제거
 as_tibble()                           # tibble로 변환

## ex) yoon<-readLines("클립보드")
##ribrary(dplyr)
##yoon2 <- yoon %>%
##  str_replace_all("[^가-힣]", " ") %>%  
##  str_squish() %>%                      
##  as_tibble()             

# ------------------------------------------------------------------------
iris             # data frame 출력
as_tibble(iris)  # tibble 구조로 변환

#백터형식이라 티블로 바꿔줌, 장점은 앞에 10개만 나오고 나머지는 생략된다.
#데이터를 바꾼게 아닌 형식만 바꿈

# 01-2 --------------------------------------------------------------------

text <- tibble(value = "대한민국은 민주공화국이다. 대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다.")
text


# ------------------------------------------------------------------------
library(tidytext)

# 문장 기준 토큰화
text %>%
  unnest_tokens(input = value,        # 토큰화할 텍스트    #value는 변수명
                output = word,        # 출력 변수명
                token = "sentences")  # "sentences"문장 기준


# ------------------------------------------------------------------------
# 띄어쓰기 기준 토큰화
text %>%
  unnest_tokens(input = value,          
                output = word,
                token = "words")      # "words"띄어쓰기 기준(가장 쉬운 방법)


# ------------------------------------------------------------------------
# 문자 기준 토큰화
text %>%
  unnest_tokens(input = value,
                output = word,
                token = "characters")  # "characters"문자 기준


# 단어 빈도 구하기 -(count)------------------------------------------------
word_space <- moon %>%
  unnest_tokens(input = value,
                output = word,
                token = "words")
word_space

# 01-3 --------------------------------------------------------------------
word_space <- word_space %>%       #가장 많이 사용되는 단어들
 count(word, sort = T)

word_space


# ------------------------------------------------------------------------
str_count("배")
str_count("사과")


# ------------------------------------------------------------------------
# 두 글자 이상만 남기기
word_space <- word_space %>%      #글자 수를 세는 거
 filter(str_count(word) > 1)     #글자 수가 한개 이상인 것만 추출

word_space

# ------------------------------------------------------------------------
top20 <- word_space %>%          #자주 사용되는 단어 추출(상위 20개만)
 head(20)

top20

# ------------------------------------------------------------------------
#그림그리기
library(ggplot2)                                          
                                                          # fill=n을 해야 색이 나옴
ggplot(top20, aes(x = reorder(word, n), y = n,fill=n)) +  # 단어 빈도순 정렬
 geom_col() +                                       
 coord_flip()+                                            # 회전                                                     
 scale_fill_gradient(low="red",high="gold")              #reorder로 정렬해서 그릴 수 있다(정렬하지 않으면 섞여서, 정렬하면 많은거부터)
     

# ------------------------------------------------------------------------
ggplot(top20, aes(x = reorder(word, n), y = n)) +
 geom_col() +
 coord_flip() +
 geom_text(aes(label = n), hjust = -0.3) +            # 막대 밖 빈도 표시
  
 labs(title = "문재인 대통령 출마 연설문 단어 빈도",  # 그래프 제목
      x = NULL, y = NULL) +                           # 축 이름 삭제
  
 theme(title = element_text(size = 12))               # 제목 크기

# ------------------------------------------------------------------------
#워드 클라우드 만들기
#ggplot으로 그려짐
library(ggwordcloud)

ggplot(word_space, aes(label = word, size = n)) +
 geom_text_wordcloud(seed = 1234) +     
 scale_radius(limits = c(3, NA),     # limits 최소, 최대 단어 빈도
              range = c(3, 30))      # range 최소, 최대 글자 크기


# ------------------------------------------------------------------------
ggplot(word_space, 
       aes(label = word, 
           size = n, 
           col = n)) +                     # 빈도에 따라 색깔 표현
 geom_text_wordcloud(seed = 1234) +  
 scale_radius(limits = c(3, NA),
              range = c(3, 30)) +
 scale_color_gradient(low = "#66aaf2",     # 최소 빈도 색깔
                      high = "#004EA1") +  # 최고 빈도 색깔
 theme_minimal()                           # 배경 없는 테마 적용


# ------------------------------------------------------------------------
library(showtext)
library(extrafont)
font_add_google(name = "Nanum Gothic", family = "nanumgothic")
font_import(paths=NULL, recursive = TRUE, prompt=TRUE, pattern=NULL)
showtext_auto()

# ------------------------------------------------------------------------
#폰트 
font_add_google(name = "Black Han Sans", family = "blackhansans")
showtext_auto()
ggplot(word_space,
       aes(label = word,
           size = n,
           col = n)) +
  geom_text_wordcloud(seed = 1234,
                      family = "blackhansans") +  # 폰트 적용  #family에 원하는 폰트 넣으면 폰트 바뀜(font.google.com에서 폰트 찾기 가능)
  scale_radius(limits = c(3, NA),
               range = c(3, 30)) +
  scale_color_gradient(low = "#66aaf2",
                       high = "#004EA1") +
  theme_minimal()


# ------------------------------------------------------------------------
font_add_google(name = "Gamja Flower", family = "gamjaflower")
showtext_auto()
ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  
  labs(title = "문재인 대통령 출마 연설문 단어 빈도",
       x = NULL, y = NULL) +
  
  theme(title = element_text(size = 12),
        text = element_text(family = "gamjaflower"))  # 폰트 적용





###예시-------------------------------------------------
library(stringr)
library(dplyr)
library(tidytext)
library(ggplot2)
yoon <- readLines("clipboard")
yoon2 <- yoon %>%
  str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기
  str_squish() %>%                      # 연속된 공백 제거
  as_tibble()  
yoon2 %>%
  unnest_tokens(input = value,
                output = word,
                token = "words") -> daegu3
daegu3 %>% 
  count(word, sort=T) %>% 
  filter(str_count(word)>1) -> daegu4
library(ggplot2)
daegu4 %>% 
  head(20) %>% 
  ggplot(aes(x=word, y=n, fill=n))+geom_col()+coord_flip()
daegu4 %>% 
  head(20) %>% 
  ggplot(aes(reorder(x=word,n), y=n, fill=n))+geom_col()+coord_flip()
library(ggwordcloud)
ggplot(daegu4, aes(label = word, size = n)) +
  geom_text_wordcloud(seed = 1234) +
  scale_radius(limits = c(5, NA),     # 최소, 최대 단어 빈도
               range = c(3, 30)) 
##------------------------------------------------------------------------

# 02-1 --------------------------------------------------------------------

install.packages("multilinguer")
library(multilinguer)
install_jdk()

install.packages(c("stringr", "hash", "tau", "Sejong", "RSQLite", "devtools"),
                 type = "binary")

install.packages("remotes")
remotes::install_github("haven-jeon/KoNLP",
                        upgrade = "never",
                        INSTALL_opts = c("--no-multiarch"))

library(KoNLP)

useNIADic()



# -------------------------------------------------------------------------
library(dplyr)
text <- tibble(
  value = c("대한민국은 민주공화국이다.",
            "대한민국의 주권은 국민에게 있고, 모든 권력은 국민으로부터 나온다."))
text

extractNoun(text$value)

# -------------------------------------------------------------------------
library(tidytext)
text %>%
  unnest_tokens(input = value,        # 분석 대상
                output = word,        # 출력 변수명
                token = extractNoun)  # 토큰화 함수


# -------------------------------------------------------------------------
# 문재인 대통령 연설문 불러오기
raw_moon <- readLines("speech_moon.txt", encoding = "UTF-8")

# 기본적인 전처리
library(stringr)

moon <- raw_moon %>%
  str_replace_all("[^가-힣]", " ") %>%  # 한글만 남기기
  str_squish() %>%                      # 중복 공백 제거
  as_tibble()                           # tibble로 변환

# 명사 기준 토큰화
word_noun <- moon %>%
  unnest_tokens(input = value,
                output = word,
                token = extractNoun)

word_noun


# 02-2 --------------------------------------------------------------------

word_noun <- word_noun %>%
  count(word, sort = T) %>%    # 단어 빈도 구해 내림차순 정렬
  filter(str_count(word) > 1)  # 두 글자 이상만 남기기

word_noun


# -------------------------------------------------------------------------
# 상위 20개 단어 추출
top20 <- word_noun %>%
  head(20)

# 막대 그래프 만들기
library(ggplot2)
ggplot(top20, aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.3) +
  labs(x = NULL) +
  theme(text = element_text(family = "nanumgothic"))


# -------------------------------------------------------------------------
# 폰트 설정
library(showtext)
font_add_google(name = "Black Han Sans", family = "blackhansans")
showtext_auto()

library(ggwordcloud)
ggplot(word_noun, aes(label = word, size = n, col = n)) +
  geom_text_wordcloud(seed = 1234, family = "blackhansans") +
  scale_radius(limits = c(3, NA),
               range = c(3, 15)) +
  scale_color_gradient(low = "#66aaf2", high = "#004EA1") +
  theme_minimal()


# 02-3 --------------------------------------------------------------------

sentences_moon <- raw_moon %>%
  str_squish() %>%
  as_tibble() %>%
  unnest_tokens(input = value,
                output = sentence,
                token = "sentences")

sentences_moon


# -------------------------------------------------------------------------
str_detect("치킨은 맛있다", "치킨")
str_detect("치킨은 맛있다", "피자")


# -------------------------------------------------------------------------
sentences_moon %>%
  filter(str_detect(sentence, "국민"))


# -------------------------------------------------------------------------
sentences_moon %>%
  filter(str_detect(sentence, "일자리"))


